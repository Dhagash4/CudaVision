{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Forward Pass"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Layer Forward"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def linear_layer_forward(x: np.ndarray,w: np.ndarray,b: np.ndarray):\n",
    "\n",
    "    '''\n",
    "    Computes forward pass for simple linear layer \n",
    "    \n",
    "\n",
    "    Input: \n",
    "    x: Numpy array containing input data, N x H x W we flatten last two dimensions so its N x D\n",
    "    w: Numpy array of weights, D x M\n",
    "    b: Numpy array of bias, M x\n",
    "    \n",
    "    Output:\n",
    "\n",
    "\n",
    "    output: Numpy array after matrix multiplication, N x M\n",
    "    cache: Cache of input params to be used in backward pass\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #Initialize weights and bias for this layer\n",
    "\n",
    "    output = (x @ w) + b\n",
    "    \n",
    "    cache = (x,w,b)\n",
    "    return output, cache"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sigmoid Function Forward"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def sigmoid_activation_forward(x: np.ndarray) -> (np.ndarray):\n",
    "\n",
    "    '''\n",
    "    \n",
    "    Apply sigmoid function on given input\n",
    "    \n",
    "    Input:\n",
    "\n",
    "    x: Numpy array, NxD\n",
    "\n",
    "    Output:\n",
    "\n",
    "    output: Numpy array after sigmoid activation , NxD\n",
    "\n",
    "    '''\n",
    "\n",
    "    output  = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    cache = x\n",
    "\n",
    "    return output,cache"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax Layer Forward"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def softmax_activation_forward(x: np.ndarray) -> (np.ndarray):\n",
    "    \n",
    "    '''\n",
    "    Apply softmax function on given input\n",
    "    \n",
    "    Input:\n",
    "\n",
    "    x: Numpy array, NxD\n",
    "\n",
    "    Output:\n",
    "\n",
    "    output: Numpy array after softmax activation , NxD\n",
    "\n",
    "    '''\n",
    "\n",
    "    normalize_input = x/np.max(x)\n",
    "    output = (np.exp(normalize_input).T / np.sum(np.exp(normalize_input), axis=1)).T\n",
    "\n",
    "    cache = x\n",
    "\n",
    "    return output,cache"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def MSE_loss(y_pred: np.ndarray, y: np.ndarray) -> float:\n",
    "    \n",
    "    '''\n",
    "    Computes Mean squared error loss\n",
    "    \n",
    "    Input:\n",
    "\n",
    "    y: Numpy array containg ground truth labels, NxD\n",
    "    y_pred: Numpy array from network, NxD\n",
    "\n",
    "    Output:\n",
    "\n",
    "    loss: scaler, mean squared error loss\n",
    "    gradient: gradient w.r.t y_pred used for backward pass\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    N = y.shape[0]\n",
    "    k = y.shape[1]\n",
    "    \n",
    "    loss =  (np.sum(np.sum((y_pred-y)**2,axis=1)/k))/N\n",
    "    \n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Backward Pass"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Backward Layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def linear_layer_backward(upstream,cache):\n",
    "\n",
    "    x,w,b = cache  \n",
    "    dx = (upstream @ w.T).reshape(x.shape)\n",
    "    dw = (upstream.T @ x).reshape(w.shape)\n",
    "    db = np.sum(upstream)\n",
    "\n",
    "\n",
    "    return dx,dw, db"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sigmoid Backward Layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def sigmoid_layer_backward(upstream,cache):\n",
    "\n",
    "    x = cache\n",
    "\n",
    "    sigmoid = 1 /( 1 + np.exp(-x))\n",
    "    sigmoid_derivative = (sigmoid * (1 - sigmoid))\n",
    "\n",
    "    dx = (upstream * sigmoid_derivative)\n",
    "\n",
    "    return dx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax Activation Layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def softmax_layer_backward(cache,y):\n",
    "\n",
    "    x = cache\n",
    "        \n",
    "    dx = np.zeros((x.shape[0],x.shape[1]))\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        \n",
    "        x_vector = x[i].reshape((x[i].shape[0],1))\n",
    "        x_matrix = np.tile(x_vector,x[i].shape[0])\n",
    "        softmax_derivative = np.sum((x[i]-y[i]) * np.diag(x[i]) - (x_matrix * np.transpose(x_matrix)),axis=1)\n",
    "        dx[i] = softmax_derivative\n",
    "\n",
    "    return dx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data', train=False, transform=transforms.ToTensor(), download=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "images_train = len(train_dataset)\n",
    "images_test  = len(test_dataset)\n",
    "\n",
    "print(\"Images Train %d\"%(images_train))\n",
    "print(\"Images Test %d\"%(images_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Images Train 60000\n",
      "Images Test 10000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "BATCH_SIZE = 20\n",
    "SHUFFLE = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=SHUFFLE)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=SHUFFLE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One Hot Encoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def array_to_one_hot(arr, C):\n",
    "    one_hot = np.zeros((arr.shape[0], C))\n",
    "\n",
    "    for i in range(arr.shape[0]):\n",
    "\n",
    "        idx = int(arr[i])\n",
    "        one_hot[i, idx] = 1\n",
    "\n",
    "    return one_hot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class TwoLayerModel(object):\n",
    "\n",
    "    def __init__(self,input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
    "\n",
    "        self.params = {}\n",
    "\n",
    "\n",
    "        #Initialize weights and bias\n",
    "\n",
    "        self.params['w1'] = np.random.uniform(-0.5, 0.5, (input_dim, hidden_dim))\n",
    "        self.params['b1'] = np.zeros((hidden_dim,))\n",
    "        self.params['w2'] = np.random.uniform(-0.5,0.5, (hidden_dim,output_dim))\n",
    "        self.params['b2'] = np.zeros((output_dim,))\n",
    "        \n",
    "        pass\n",
    "    \n",
    "\n",
    "    # def forward\n",
    "\n",
    "    def loss(self,X,y=None):\n",
    "\n",
    "\n",
    "        #Forward Pass \n",
    "\n",
    "        x,cache_linear_1 = linear_layer_forward(X,self.params['w1'],self.params['b1'])\n",
    "        x,cache_sigmoid = sigmoid_activation_forward(x)\n",
    "        x,cache_linear_2 = linear_layer_forward(x,self.params['w2'],self.params['b2'])\n",
    "        y_pred,cache_softmax = softmax_activation_forward(x)\n",
    "\n",
    "\n",
    "        if y is None:\n",
    "            return y_pred\n",
    "\n",
    "        loss = 0\n",
    "        grads = {}\n",
    "\n",
    "        # Calculate loss\n",
    "\n",
    "        loss = MSE_loss(y_pred,y)\n",
    "\n",
    "\n",
    "        #Backward pass\n",
    "    \n",
    "        dsoftmax_activation = softmax_layer_backward(cache_softmax,y)\n",
    "        dx2,dw2,db2 = linear_layer_backward(dsoftmax_activation,cache_linear_2)\n",
    "        dsigmoid_activation = sigmoid_layer_backward(dx2,cache_sigmoid)\n",
    "        dx1,dw1,db1 = linear_layer_backward(dsigmoid_activation,cache_linear_1)\n",
    "\n",
    "\n",
    "        grads['w1'] = dw1\n",
    "        grads['w2'] = dw2\n",
    "        grads['b1'] = db1\n",
    "        grads['b2'] = db2\n",
    "\n",
    "        return loss,grads\n",
    "\n",
    "\n",
    "    def check_accuracy(self,data,labels):\n",
    "\n",
    "        y_pred = []\n",
    "        probs = self.loss(data)\n",
    "\n",
    "        y_pred.append(np.argmax(probs,axis=1))\n",
    "\n",
    "        y_pred = np.hstack(y_pred)\n",
    "        \n",
    "        acc = np.mean(y_pred == labels)\n",
    "\n",
    "        return acc\n",
    "    \n",
    "    def train(self,dataloader,iterations,learning_rate):\n",
    "\n",
    "\n",
    "        for t in range(iterations):\n",
    "            loss = 0.0\n",
    "            grads_train = {}\n",
    "            acc = 0.0\n",
    "            for i,(imgs,labels) in enumerate(dataloader):\n",
    "\n",
    "                X_batch = imgs.squeeze(1).reshape((imgs.shape[0],np.prod(imgs.shape[1:],axis=0))).cpu().numpy()\n",
    "                y_batch = array_to_one_hot(labels.cpu().numpy(),10)\n",
    "\n",
    "                loss_batch,grad_batch = self.loss(X_batch,y_batch)\n",
    "\n",
    "                loss += loss_batch\n",
    "\n",
    "                for k,v in grad_batch.items():\n",
    "                    \n",
    "                    partial_derivative = grad_batch[k]\n",
    "\n",
    "                    update = self.params[k] - (learning_rate * partial_derivative)\n",
    "\n",
    "                    self.params[k] = update\n",
    "\n",
    "                acc_batch = self.check_accuracy(X_batch,labels.cpu().numpy())\n",
    "\n",
    "                acc += acc_batch\n",
    "\n",
    "            print(acc/len(dataloader))\n",
    "            \n",
    "            print(\"Iteration Loss %f\"%(loss))\n",
    "\n",
    "\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "model = TwoLayerModel(784,64,10)\n",
    "model.train(train_loader,4,1e-2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/dhagash/anaconda3/envs/dl/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in multiply\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/dhagash/anaconda3/envs/dl/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in subtract\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.09870000000000094\n",
      "Iteration Loss nan\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-62933304d4b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwoLayerModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-63100c183db2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataloader, iterations, learning_rate)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_to_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-63100c183db2>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mdx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_layer_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsoftmax_activation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_linear_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdsigmoid_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid_layer_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_sigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mdx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_layer_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsigmoid_activation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_linear_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-1b22a2f3fe29>\u001b[0m in \u001b[0;36mlinear_layer_backward\u001b[0;34m(upstream, cache)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mupstream\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mupstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing/Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('dl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 3,
  "interpreter": {
   "hash": "f7881038a8c0c2c5168ac80e20ff544f471faf7d5bc66c5653256549b0169354"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}