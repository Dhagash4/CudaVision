{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer_forward(x: np.ndarray,w: np.ndarray,b: np.ndarray):\n",
    "\n",
    "    '''\n",
    "    Computes forward pass for simple linear layer \n",
    "    \n",
    "\n",
    "    Input: \n",
    "    x: Numpy array containing input data, N x H x W we flatten last two dimensions so its N x D\n",
    "    w: Numpy array of weights, D xM\n",
    "    b: Numpy array of bias, M x\n",
    "    \n",
    "    Output:\n",
    "\n",
    "\n",
    "    output: Numpy array after matrix multiplication, N x M\n",
    "    cache: Cache of input params to be used in backward pass\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #Initialize weights and bias for this layer\n",
    "\n",
    "    output = (x @ w) + b\n",
    "    \n",
    "    cache = (x,w,b)\n",
    "    return output, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation_forward(x: np.ndarray) -> (np.ndarray):\n",
    "\n",
    "    '''\n",
    "    \n",
    "    Apply sigmoid function on given input\n",
    "    \n",
    "    Input:\n",
    "\n",
    "    x: Numpy array, NxD\n",
    "\n",
    "    Output:\n",
    "\n",
    "    output: Numpy array after sigmoid activation , NxD\n",
    "\n",
    "    '''\n",
    "\n",
    "    #Check if we need to normalize the input before passing to sigmoid\n",
    "\n",
    "    output  = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    cache = x\n",
    "\n",
    "    return output,cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Layer Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activation_forward(x: np.ndarray) -> (np.ndarray):\n",
    "    \n",
    "    '''\n",
    "    Apply softmax function on given input\n",
    "    \n",
    "    Input:\n",
    "\n",
    "    x: Numpy array, NxD\n",
    "\n",
    "    Output:\n",
    "\n",
    "    output: Numpy array after softmax activation , NxD\n",
    "\n",
    "    '''\n",
    "\n",
    "    normalize_input = x - np.max(x)\n",
    "\n",
    "    output = np.exp(normalize_input)/(np.sum(np.exp(normalize_input)))\n",
    "\n",
    "    cache = normalize_input\n",
    "\n",
    "    return output,cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_loss(y_pred: np.ndarray, y: np.ndarray) -> float:\n",
    "    \n",
    "    '''\n",
    "    Computes Mean squared error loss\n",
    "    \n",
    "    Input:\n",
    "\n",
    "    y: Numpy array containg ground truth labels, NxD\n",
    "    y_pred: Numpy array from network, NxD\n",
    "\n",
    "    Output:\n",
    "\n",
    "    loss: scaler, mean squared error loss\n",
    "    gradient: gradient w.r.t y_pred used for backward pass\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    N = y.shape[0]\n",
    "\n",
    "    k = y.shape[1]\n",
    "    loss = np.sum((np.sum(((y-y_pred)**2),axis=1)/k))/ N\n",
    "    gradient = (-2 * (y-y_pred)) / (k*N)\n",
    "\n",
    "    \n",
    "    return loss,gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Backward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer_backward(upstream,cache):\n",
    "\n",
    "\n",
    "\n",
    "    x,w,b = cache  \n",
    "    dx = (upstream @ w.T).reshape(x.shape)\n",
    "    dw = (upstream.T @ x).reshape(w.shape)\n",
    "    db = np.sum(upstream)\n",
    "\n",
    "\n",
    "    return dx,dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Backward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_layer_backward(upstream,cache):\n",
    "\n",
    "    x = cache\n",
    "    \n",
    "    softmax,_ = softmax_activation_forward(x)\n",
    "\n",
    "    sigmoid_derivative = (softmax * (1 - softmax))\n",
    "\n",
    "    dx = (upstream * sigmoid_derivative)\n",
    "\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_layer_backward(grad,cache):\n",
    "\n",
    "    x = cache\n",
    "\n",
    "\n",
    "    if x.shape[0] > 1:\n",
    "        \n",
    "        dx = np.zeros((x.shape[0],x.shape[1],x.shape[1]))\n",
    "        for i in range(x.shape[0]):\n",
    "            \n",
    "            x_vector = x[i].reshape((x[i].shape[0],1))\n",
    "            x_matrix = np.tile(x_vector,x[i].shape[0])\n",
    "            softmax_derivative = np.diag(x[i]) - (x_matrix * np.transpose(x_matrix))\n",
    "            dx[i] = softmax_derivative\n",
    "            \n",
    "            \n",
    "        dx = np.einsum('ijk,ji->ij', dx,grad.T)\n",
    "            \n",
    "    else:\n",
    "        x_vector = x.reshape((x.shape[1], 1))\n",
    "        x_matrix = np.tile(x_vector, x.shape)\n",
    "\n",
    "        dx = np.diag(x) - (x_matrix * np.transpose(x_matrix))\n",
    "\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data', train=False, transform=transforms.ToTensor(), download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images Train 60000\n",
      "Images Test 10000\n"
     ]
    }
   ],
   "source": [
    "images_train = len(train_dataset)\n",
    "images_test  = len(test_dataset)\n",
    "\n",
    "print(\"Images Train %d\"%(images_train))\n",
    "print(\"Images Test %d\"%(images_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "SHUFFLE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=SHUFFLE)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_one_hot(arr, C):\n",
    "    one_hot = np.zeros((arr.shape[0], C))\n",
    "\n",
    "    for i in range(arr.shape[0]):\n",
    "\n",
    "        idx = int(arr[i])\n",
    "        one_hot[i, idx] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerModel(object):\n",
    "\n",
    "    def __init__(self,input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
    "\n",
    "        self.params = {}\n",
    "\n",
    "\n",
    "        #Initialize weights and bias\n",
    "\n",
    "        self.params['w1'] = np.random.uniform(-0.5,0.5,(input_dim,hidden_dim))\n",
    "        self.params['b1'] = np.zeros((hidden_dim,))\n",
    "        self.params['w2'] = np.random.uniform(-0.5,0.5,(hidden_dim,output_dim))\n",
    "        self.params['b2'] = np.zeros((output_dim,))\n",
    "        self.reg = 0.2\n",
    "\n",
    "        pass\n",
    "\n",
    "    def loss(self,X,y):\n",
    "\n",
    "\n",
    "        #Forward Pass \n",
    "\n",
    "        x,cache_linear_1 = linear_layer_forward(X,self.params['w1'],self.params['b1'])\n",
    "        x,cache_sigmoid = sigmoid_activation_forward(x)\n",
    "        x,cache_linear_2 = linear_layer_forward(x,self.params['w2'],self.params['b2'])\n",
    "        \n",
    "        y_pred,cache_softmax = softmax_activation_forward(x)\n",
    "        \n",
    "\n",
    "\n",
    "        loss = 0\n",
    "        grads = {}\n",
    "\n",
    "        # Calculate loss\n",
    "\n",
    "        loss,upstreamgrad_loss = MSE_loss(y_pred,y)\n",
    "        loss += self.reg * 0.5 * \\\n",
    "            (np.sum(self.params['w1']**2) + np.sum(self.params['w2']**2))\n",
    "\n",
    "        #Backward pass\n",
    "    \n",
    "        dsoftmax_activation = softmax_layer_backward(upstreamgrad_loss,cache_softmax)\n",
    "        dx2,dw2,db2 = linear_layer_backward(dsoftmax_activation,cache_linear_2)\n",
    "        dsigmoid_activation = sigmoid_layer_backward(dx2,cache_sigmoid)\n",
    "        dx1,dw1,db1 = linear_layer_backward(dsigmoid_activation,cache_linear_1)\n",
    "\n",
    "\n",
    "        grads['w1'] = dw1\n",
    "        grads['w2'] = dw2\n",
    "        grads['b1'] = db1\n",
    "        grads['b2'] = db2\n",
    "\n",
    "        return loss,grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "\n",
    "    def __init__(self,model,train_dataloader,test_dataloader,num_iterations,lr) -> None:\n",
    "\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.num_iterations = num_iterations\n",
    "        self.learning_rate = lr\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Set up some book-keeping variables for optimization. Don't call this\n",
    "        manually.\n",
    "        \"\"\"\n",
    "        # Set up some variables for book-keeping\n",
    "        self.epoch = 0\n",
    "        self.best_val_acc = 0\n",
    "        self.best_params = {}\n",
    "        self.loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.val_acc_history = []\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def array_to_one_hot(arr, C):\n",
    "     \n",
    "        one_hot = np.zeros((arr.shape[0], C))\n",
    "\n",
    "        for i in range(arr.shape[0]):\n",
    "\n",
    "            idx = int(arr[i])\n",
    "            one_hot[i, idx] = 1\n",
    "\n",
    "        return one_hot\n",
    "    \n",
    "    def _step(self,X_batch,y_batch):\n",
    "\n",
    "        loss,grads = self.model.loss(X_batch,y_batch)\n",
    "        self.loss_history.append(loss)\n",
    "\n",
    "        for p,w in self.model.params.items():\n",
    "            \n",
    "            dw = grads[p]\n",
    "            next_w = self.update_rule(w, dw, self.learning_rate)\n",
    "            self.model.params[p] = next_w\n",
    "            # self.optim_configs[p] = next_config\n",
    "\n",
    "    \n",
    "    def update_rule(self,w,dw,lr):\n",
    "        \n",
    "        \n",
    "        w -= lr * dw\n",
    "        \n",
    "        return w\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "\n",
    "        for t in range(self.num_iterations):\n",
    "\n",
    "            for i, (imgs, labels) in enumerate(train_loader):\n",
    "                \n",
    "                X_batch = (imgs.squeeze(1).reshape((imgs.shape[0], np.prod(imgs.shape[1:], axis=0)))).cpu().numpy().astype(np.float128)\n",
    "                labels = array_to_one_hot(labels.cpu().numpy(), 10)\n",
    "\n",
    "                self._step(X_batch,labels)\n",
    "\n",
    "           \n",
    "\n",
    "                print(\n",
    "                    \"(Iteration %d / %d) loss: %f\"\n",
    "                    % (t + 1, self.num_iterations, self.loss_history[-1])\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = Solver(TwoLayerModel(784,64,10),train_loader,test_loader,1,1e-3)\n",
    "\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('dl': conda)",
   "name": "python3710jvsc74a57bd0f7881038a8c0c2c5168ac80e20ff544f471faf7d5bc66c5653256549b0169354"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}