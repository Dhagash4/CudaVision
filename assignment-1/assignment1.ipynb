{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Tuple\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Forward Pass"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Layer Forward"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def linear_layer_forward(x: np.ndarray,w: np.ndarray,b: np.ndarray):\n",
    "\n",
    "    '''\n",
    "    Computes forward pass for simple linear layer \n",
    "    \n",
    "\n",
    "    Input: \n",
    "    x: Numpy array containing input data, N x H x W we flatten last two dimensions so its N x D\n",
    "    w: Numpy array of weights, D xM\n",
    "    b: Numpy array of bias, M x\n",
    "    \n",
    "    Output:\n",
    "\n",
    "\n",
    "    output: Numpy array after matrix multiplication, N x M\n",
    "    cache: Cache of input params to be used in backward pass\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #Initialize weights and bias for this layer\n",
    "\n",
    "    output = (x @ w) + b\n",
    "    \n",
    "    cache = (x,w,b)\n",
    "    return output, cache"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sigmoid Function Forward"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def sigmoid_activation_forward(x: np.ndarray) -> (np.ndarray):\n",
    "\n",
    "    '''\n",
    "    \n",
    "    Apply sigmoid function on given input\n",
    "    \n",
    "    Input:\n",
    "\n",
    "    x: Numpy array, NxD\n",
    "\n",
    "    Output:\n",
    "\n",
    "    output: Numpy array after sigmoid activation , NxD\n",
    "\n",
    "    '''\n",
    "\n",
    "    #Check if we need to normalize the input before passing to sigmoid\n",
    "\n",
    "    output  = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    cache = x\n",
    "\n",
    "    return output,cache"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax Layer Forward"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def softmax_activation_forward(x: np.ndarray) -> (np.ndarray):\n",
    "    \n",
    "    '''\n",
    "    Apply softmax function on given input\n",
    "    \n",
    "    Input:\n",
    "\n",
    "    x: Numpy array, NxD\n",
    "\n",
    "    Output:\n",
    "\n",
    "    output: Numpy array after softmax activation , NxD\n",
    "\n",
    "    '''\n",
    "\n",
    "    normalize_input = x - np.max(x)\n",
    "\n",
    "    output = np.exp(normalize_input)/(np.sum(np.exp(normalize_input)))\n",
    "\n",
    "    cache = normalize_input\n",
    "\n",
    "    return output,cache"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def MSE_loss(y_pred: np.ndarray, y: np.ndarray) -> float:\n",
    "    \n",
    "    '''\n",
    "    Computes Mean squared error loss\n",
    "    \n",
    "    Input:\n",
    "\n",
    "    y: Numpy array containg ground truth labels, NxD\n",
    "    y_pred: Numpy array from network, NxD\n",
    "\n",
    "    Output:\n",
    "\n",
    "    loss: scaler, mean squared error loss\n",
    "    gradient: gradient w.r.t y_pred used for backward pass\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    N = y.shape[0]\n",
    "\n",
    "    k = y.shape[1]\n",
    "    loss = np.sum((np.sum(((y-y_pred)**2),axis=1)/k))/ N\n",
    "    gradient = (-2 * (y-y_pred)) / (k*N)\n",
    "\n",
    "    \n",
    "    return loss,gradient"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Backward Pass"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Backward Layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def linear_layer_backward(upstream,cache):\n",
    "\n",
    "\n",
    "\n",
    "    x,w,b = cache  \n",
    "\n",
    "\n",
    "    print(upstream.shape,x.shape,w.shape)\n",
    "    dx = (upstream @ w.T).reshape(x.shape)\n",
    "    dw = (upstream.T @ x).reshape(w.shape)\n",
    "    db = np.sum(upstream)\n",
    "\n",
    "\n",
    "    return dx,dw, db"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sigmoid Backward Layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def sigmoid_layer_backward(upstream,cache):\n",
    "\n",
    "    x = cache\n",
    "    \n",
    "    softmax,_ = softmax_activation_forward(x)\n",
    "\n",
    "    sigmoid_derivative = (softmax * (1 - softmax))\n",
    "\n",
    "    dx = (upstream * sigmoid_derivative)\n",
    "\n",
    "    return dx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax Activation Layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def softmax_layer_backward(grad,cache):\n",
    "\n",
    "    x = cache\n",
    "\n",
    "    dx = np.zeros((x.shape[0],x.shape[1],x.shape[1]))\n",
    "\n",
    "    if x.shape[0] > 1:\n",
    "        \n",
    "        for i in range(x.shape[0]):\n",
    "            \n",
    "            x_vector = x[i].reshape((x[i].shape[0],1))\n",
    "            x_matrix = np.tile(x_vector,x[i].shape[0])\n",
    "            softmax_derivative = np.diag(x[i]) - (x_matrix * np.transpose(x_matrix))\n",
    "            dx[i] = softmax_derivative\n",
    "            # print(softmax_derivative.shape)\n",
    "            \n",
    "        dx = np.einsum('ijk,ji->ij', dx,grad.T)\n",
    "            \n",
    "    else:\n",
    "        x_vector = x.reshape((x.shape[1], 1))\n",
    "        x_matrix = np.tile(x_vector, x.shape)\n",
    "\n",
    "        dx = np.diag(x) - (x_matrix * np.transpose(x_matrix))\n",
    "\n",
    "    return dx\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class TwoLayerModel(object):\n",
    "\n",
    "    def __init__(self,input_dim: int, hidden_dim: int, output_dim: int) -> None:\n",
    "\n",
    "        self.params = {}\n",
    "\n",
    "\n",
    "        #Initialize weights and bias\n",
    "\n",
    "        self.params['w1'] = np.random.uniform(-0.5,0.5,(input_dim,hidden_dim))\n",
    "        self.params['b1'] = np.zeros((hidden_dim,))\n",
    "        self.params['w2'] = np.random.uniform(-0.5,0.5,(hidden_dim,output_dim))\n",
    "        self.params['b2'] = np.zeros((output_dim,))\n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "    def loss(self,X,y):\n",
    "\n",
    "\n",
    "        #Forward Pass \n",
    "\n",
    "        x,cache_linear_1 = linear_layer_forward(X,self.params['w1'],self.params['b1'])\n",
    "        x,cache_sigmoid = sigmoid_activation_forward(x)\n",
    "        x,cache_linear_2 = linear_layer_forward(x,self.params['w2'],self.params['b2'])\n",
    "        \n",
    "        y_pred,cache_softmax = softmax_activation_forward(x)\n",
    "        \n",
    "\n",
    "\n",
    "        loss = 0\n",
    "        grads = {}\n",
    "\n",
    "        # Calculate loss\n",
    "\n",
    "        loss,upstreamgrad_loss = MSE_loss(y_pred,y)\n",
    "\n",
    "\n",
    "        #Backward pass\n",
    "    \n",
    "        dsoftmax_activation = softmax_layer_backward(upstreamgrad_loss,cache_softmax)\n",
    "        dx2,dw2,db2 = linear_layer_backward(dsoftmax_activation,cache_linear_2)\n",
    "        dsigmoid_activation = sigmoid_layer_backward(dx2,cache_sigmoid)\n",
    "        dx1,dw1,db1 = linear_layer_backward(dsigmoid_activation,cache_linear_1)\n",
    "\n",
    "\n",
    "        grads['w1'] = dw1\n",
    "        grads['w2'] = dw2\n",
    "        grads['b1'] = db1\n",
    "        grads['b2'] = db2\n",
    "\n",
    "        return loss,grads"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "X = np.random.random((20,784))\n",
    "y = np.zeros((20,10))\n",
    "\n",
    "model = TwoLayerModel(784,64,10)\n",
    "\n",
    "\n",
    "\n",
    "loss,grad = model.loss(X,y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(20, 10) (20, 64) (64, 10)\n",
      "(20, 64) (20, 28, 28) (784, 64)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 28 is different from 20)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1dd128df0d38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-d85175b15d3f>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mdx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_layer_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsoftmax_activation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_linear_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mdsigmoid_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid_layer_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_sigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mdx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_layer_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsigmoid_activation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_linear_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-49b51e8cfa0c>\u001b[0m in \u001b[0;36mlinear_layer_backward\u001b[0;34m(upstream, cache)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mupstream\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mupstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 28 is different from 20)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing/Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('dl': conda)"
  },
  "interpreter": {
   "hash": "f7881038a8c0c2c5168ac80e20ff544f471faf7d5bc66c5653256549b0169354"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}