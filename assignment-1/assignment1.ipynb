{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from utils import array_to_one_hot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataloader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data', train=False, transform=transforms.ToTensor(), download=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "BATCH_SIZE = 20\n",
    "SHUFFLE = True\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=SHUFFLE)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=SHUFFLE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class TwoLayerNet():\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "\n",
    "        self.w1 = np.random.uniform(-0.5, 0.5, (input_dim, hidden_dim))\n",
    "        self.w2 = np.random.uniform(-0.5, 0.5, (hidden_dim, output_dim))\n",
    "\n",
    "    def sigmoid_function(self, x: np.ndarray):\n",
    "\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_function_backward(self, x: np.ndarray):\n",
    "\n",
    "        output = self.sigmoid_function(x) * (1 - self.sigmoid_function(x))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def softmax_function(self, x: np.ndarray):\n",
    "\n",
    "        exp = np.exp(x)\n",
    "        sumexp = np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "        output = exp/(sumexp)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def softmax_function_backward(self, x: np.ndarray, y: np.ndarray):\n",
    "\n",
    "        s = self.y_pred\n",
    "        output = np.zeros((x.shape[0], x.shape[1]))\n",
    "\n",
    "        for i in range(x.shape[0]):\n",
    "\n",
    "            s_vector = s[i].reshape((s[i].shape[0], 1))\n",
    "            s_matrix = np.tile(s_vector, s[i].shape[0])\n",
    "            softmax_derivative = np.sum(\n",
    "                (s[i]-y[i]) * (np.diag(s[i]) - (s_matrix * np.transpose(s_matrix))), axis=1)\n",
    "            output[i] = softmax_derivative\n",
    "\n",
    "        return output\n",
    "\n",
    "    def linear_backward_x(self, upstream_grad: np.ndarray, w: np.ndarray):\n",
    "\n",
    "        dx = np.dot(upstream_grad, w.T)\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def linear_backward_w(self, upstream_grad, x):\n",
    "\n",
    "        dw = np.dot(x.T, upstream_grad)\n",
    "\n",
    "        return dw\n",
    "\n",
    "    def forward(self, X: np.ndarray):\n",
    "\n",
    "        self.x = X\n",
    "        self.layer_1 = self.x @ self.w1\n",
    "        self.sigmoid_layer = self.sigmoid_function(self.layer_1)\n",
    "        self.layer_2 = self.sigmoid_layer @ self.w2\n",
    "        self.y_pred = self.softmax_function(self.layer_2)\n",
    "\n",
    "        return self.y_pred\n",
    "\n",
    "    def backward(self, y: np.ndarray):\n",
    "\n",
    "        softmax_backward = self.softmax_function_backward(self.layer_2, y)\n",
    "        linear_layer2_backward_x = self.linear_backward_x(softmax_backward, self.w2)\n",
    "        sigmoid_backward = self.sigmoid_function_backward(self.layer_1)\n",
    "        sigmoid_backward = linear_layer2_backward_x * sigmoid_backward\n",
    "\n",
    "        dw2 = 1 /y.shape[0] * self.linear_backward_w(softmax_backward, self.sigmoid_layer)\n",
    "        dw1 = 1 / y.shape[0] * self.linear_backward_w(sigmoid_backward, self.x)\n",
    "\n",
    "        #Update gradients\n",
    "\n",
    "        self.w1 = self.w1 - (self.learning_rate * dw1)\n",
    "        self.w2 = self.w2 - (self.learning_rate * dw2)\n",
    "\n",
    "\n",
    "    def loss_mse(self, y_pred: np.ndarray, y: np.ndarray):\n",
    "\n",
    "        return np.mean((y_pred - y) ** 2)\n",
    "\n",
    "    def train(self, num_iterations, train_dataloader, learning_rate):\n",
    "\n",
    "        loss_train = []\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        progress_bar = tqdm(range(num_iterations), total=num_iterations)\n",
    "        loss_train = []\n",
    "\n",
    "        for t in progress_bar:\n",
    "\n",
    "            loss = 0.0\n",
    "\n",
    "            for imgs, labels in train_dataloader:\n",
    "\n",
    "                X_batch = imgs.squeeze().reshape(\n",
    "                    (imgs.shape[0], np.prod(imgs.shape[1:], axis=0))).numpy()\n",
    "                y_batch = array_to_one_hot(labels.numpy(), 10)\n",
    "\n",
    "                output = self.forward(X_batch)\n",
    "\n",
    "                loss_batch = self.loss_mse(output, y_batch)\n",
    "\n",
    "                self.backward(y_batch)\n",
    "\n",
    "                loss += loss_batch\n",
    "            loss_train.append(loss)\n",
    "            progress_bar.set_description(f\"Iter {t+1}: loss {loss:.5f}. \")\n",
    "    \n",
    "\n",
    "        return loss_train\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model = TwoLayerNet(784,64,10)\n",
    "\n",
    "loss = model.train(1000,train_loader,1e-3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Iter 36: loss 142.58491. :   7%|â–‹         | 36/500 [04:41<59:13,  7.66s/it]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test(test_dataloader, model):\n",
    "\n",
    "    correct_predicted = 0\n",
    "\n",
    "    for imgs, labels in test_dataloader:\n",
    "\n",
    "        X_batch = imgs.squeeze().reshape(\n",
    "            (imgs.shape[0], np.prod(imgs.shape[1:], axis=0))).numpy()\n",
    "        \n",
    "\n",
    "        output = model.forward(X_batch).numpy()\n",
    "\n",
    "        predictions = np.argmax(output, axis=1)\n",
    "\n",
    "        correct_batch = np.count_nonzero(predictions == labels.numpy())\n",
    "\n",
    "        correct_predicted += correct_batch\n",
    "    acc = (correct_batch / (len(test_loader))) * 100\n",
    "    return acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "acc = test(test_loader, model)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('dl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 3,
  "interpreter": {
   "hash": "f7881038a8c0c2c5168ac80e20ff544f471faf7d5bc66c5653256549b0169354"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}